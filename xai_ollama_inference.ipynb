{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from itertools import permutations\n",
                "from constants import *\n",
                "\n",
                "import ollama\n",
                "import os"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "RESULTS_FOLDER = \"results\"\n",
                "GENERATED_CODE_FOLDER = os.path.join(RESULTS_FOLDER, \"generated_code\")\n",
                "ACCEPTED_CODE_FOLDER = os.path.join(RESULTS_FOLDER, \"accepted_code\")\n",
                "COMPILE_FAIL_FOLDER = os.path.join(RESULTS_FOLDER, \"compile_fail\")\n",
                "SYNTAX_FAIL_FOLDER = os.path.join(RESULTS_FOLDER, \"syntax_fail\")\n",
                "FOLLOWUP_RESP_FOLDER = os.path.join(RESULTS_FOLDER, \"generated_followup\")\n",
                "MODEL = \"deepseekcoder7b\"\n",
                "LANG2LANG = list(permutations(LANGS.keys(), 2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(FOLLOWUP_RESP_FOLDER, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_prompt(system_prompt, user_prompt1, assistant_prompt, user_prompt2):\n",
                "    prompt = []\n",
                "    if system_prompt:\n",
                "        prompt.append({\"role\": \"system\", \"content\": system_prompt})\n",
                "    prompt.append({\"role\": \"user\", \"content\": user_prompt1})\n",
                "    prompt.append({\"role\": \"user\", \"content\": assistant_prompt})\n",
                "    prompt.append({\"role\": \"user\", \"content\": user_prompt2})\n",
                "    return prompt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt = \"Convert the following code from {} to {}. This is the requirement for the code - {}\\n```{}```\\n\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "compile_prompt = \"\"\"When compiling the code enclosed in backticks, the following error occurred -\n",
                "\n",
                "```{}```\n",
                "\n",
                "Explain the error in a single line.\n",
                "Then explain the fix in single line.\n",
                "Then rewrite the corrected code in a single code block.\n",
                "Ensure you write the entire code and your response should only contain one fenced code block.\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "syntax_prompt = \"\"\"When checking the syntax of code enclosed in backticks, the following issue was highlighted -\n",
                "\n",
                "```{}```\n",
                "\n",
                "Explain the error in a single line.\n",
                "Then explain the fix in single line.\n",
                "Then rewrite the corrected code in a single code block.\n",
                "Ensure you write the entire code and your response should only contain one fenced code block.\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ollama.pull(OLLAMA_TAGS[MODEL])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for TASK in TASKS.keys():\n",
                "    for idx, (lang1, lang2) in enumerate(LANG2LANG):\n",
                "        filename = f\"{MODEL}-{TASK}-{lang1.lower()}-{lang2.lower()}\"\n",
                "        compile_fail = False\n",
                "        syntax_fail = False\n",
                "        if os.path.exists(os.path.join(COMPILE_FAIL_FOLDER, f\"{filename}.txt\")):\n",
                "            compile_fail = True\n",
                "        if os.path.exists(os.path.join(SYNTAX_FAIL_FOLDER, f\"{filename}.txt\")):\n",
                "            syntax_fail = True\n",
                "        if os.path.exists(os.path.join(FOLLOWUP_RESP_FOLDER, f\"{filename}.md\")):\n",
                "            continue\n",
                "        if not compile_fail and not syntax_fail:\n",
                "            continue\n",
                "        formatted_prompt = prompt.format(\n",
                "            LANGS[lang1],\n",
                "            LANGS[lang2],\n",
                "            TASK_DESCRIPTION[TASK],\n",
                "            REFERENCE_CODE[lang1][TASK],\n",
                "        )\n",
                "        llm_response = (\n",
                "            open(os.path.join(GENERATED_CODE_FOLDER, f\"{filename}.md\"), \"r\")\n",
                "            .read()\n",
                "            .split(\"```\")\n",
                "        )\n",
                "        formatted_code = open(\n",
                "            os.path.join(ACCEPTED_CODE_FOLDER, f\"{filename}.{lang2.lower()}\"), \"r\"\n",
                "        ).read()\n",
                "        formatted_response = f\"{llm_response[0]}```{lang2.lower()}\\n{formatted_code}\\n```{llm_response[2]}\"\n",
                "        if compile_fail:\n",
                "            compile_error = open(\n",
                "                os.path.join(COMPILE_FAIL_FOLDER, f\"{filename}.txt\"), \"r\"\n",
                "            ).read()\n",
                "            formatted_followup = compile_prompt.format(compile_error)\n",
                "        elif syntax_fail:\n",
                "            syntax_error = open(\n",
                "                os.path.join(SYNTAX_FAIL_FOLDER, f\"{filename}.txt\"), \"r\"\n",
                "            ).read()\n",
                "            formatted_followup = syntax_prompt.format(syntax_error)\n",
                "        if OLLAMA_SYSTEM_PROMPT_SUPPORT[MODEL]:\n",
                "            chat_prompt = get_prompt(\n",
                "                \"You are a helpful code conversion assistant.\",\n",
                "                formatted_prompt,\n",
                "                formatted_response,\n",
                "                formatted_followup,\n",
                "            )\n",
                "        else:\n",
                "            chat_prompt = get_prompt(\n",
                "                None, formatted_prompt, formatted_response, formatted_followup\n",
                "            )\n",
                "        try:\n",
                "            rsp = ollama.chat(\n",
                "                OLLAMA_TAGS[MODEL],\n",
                "                messages=chat_prompt,\n",
                "                options={\"seed\": 42, \"timeout\": 60},\n",
                "            )\n",
                "            with open(os.path.join(FOLLOWUP_RESP_FOLDER, f\"{filename}.md\"), \"w\") as fp:\n",
                "                fp.write(rsp[\"message\"][\"content\"] + \"\\n\")\n",
                "            print(f\"completed {MODEL} {TASK} {lang1} {lang2}\")\n",
                "        except:\n",
                "            try:\n",
                "                rsp = ollama.chat(\n",
                "                    OLLAMA_TAGS[MODEL],\n",
                "                    messages=chat_prompt,\n",
                "                    options={\"seed\": 41, \"timeout\": 60},\n",
                "                )\n",
                "                with open(\n",
                "                    os.path.join(FOLLOWUP_RESP_FOLDER, f\"{filename}.md\"), \"w\"\n",
                "                ) as fp:\n",
                "                    fp.write(rsp[\"message\"][\"content\"] + \"\\n\")\n",
                "                print(f\"completed {MODEL} {TASK} {lang1} {lang2}\")\n",
                "            except:\n",
                "                with open(\n",
                "                    os.path.join(FOLLOWUP_RESP_FOLDER, f\"{filename}.md\"), \"w\"\n",
                "                ) as fp:\n",
                "                    fp.write(\"timeout\\n\")\n",
                "                print(f\"timeout {MODEL} {TASK} {lang1} {lang2}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}